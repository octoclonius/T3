{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from io import DEFAULT_BUFFER_SIZE\n",
    "from scipy.stats import entropy\n",
    "from tqdm.auto import tqdm\n",
    "import pefile\n",
    "import PySimpleGUI as sg\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "seed = 42\n",
    "# Path to original train.csv\n",
    "csv_input_train_path = 'train.csv'\n",
    "# Path to original test.csv\n",
    "csv_input_test_path = 'test.csv'\n",
    "# Path to training data\n",
    "csv_train_data_path = 'train-data.csv'\n",
    "# Path to testing data\n",
    "csv_test_data_path = 'test-data.csv'\n",
    "# Path to training labels\n",
    "csv_train_labels_path = 'train-labels.csv'\n",
    "# Path to testing labels\n",
    "csv_test_labels_path = 'test-labels.csv'\n",
    "# Path to training samples directory\n",
    "dir_train = '.\\\\train'\n",
    "# Path to testing samples directory\n",
    "dir_test = '.\\\\test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>md5</th>\n",
       "      <th>sha1</th>\n",
       "      <th>sha256</th>\n",
       "      <th>total</th>\n",
       "      <th>positives</th>\n",
       "      <th>malicious</th>\n",
       "      <th>filetype</th>\n",
       "      <th>submitted</th>\n",
       "      <th>user_id</th>\n",
       "      <th>length</th>\n",
       "      <th>entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.\\train\\1</td>\n",
       "      <td>0000e1cccca9ca2ae5d1dfb124686920</td>\n",
       "      <td>7704b8f6a9538f9bc2366696cf85eeec1f15e224</td>\n",
       "      <td>e1bde0b326a91688e8718086d06d1377394f2cb7feddec...</td>\n",
       "      <td>64</td>\n",
       "      <td>42</td>\n",
       "      <td>True</td>\n",
       "      <td>exe</td>\n",
       "      <td>40:22.2</td>\n",
       "      <td>1</td>\n",
       "      <td>67328</td>\n",
       "      <td>7.265160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>.\\train\\10</td>\n",
       "      <td>000a1a3e8204261b7408ff51ffa5b440</td>\n",
       "      <td>fb4a5e7801cf6d0f293b6051569e325c46271309</td>\n",
       "      <td>9d5fc038b67068fa5fba1bbb34b938db83b26638b4e22f...</td>\n",
       "      <td>52</td>\n",
       "      <td>23</td>\n",
       "      <td>True</td>\n",
       "      <td>exe</td>\n",
       "      <td>40:24.5</td>\n",
       "      <td>1</td>\n",
       "      <td>773024</td>\n",
       "      <td>7.935332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>.\\train\\100</td>\n",
       "      <td>013539d21eb2a5db8d0554affb3541b8</td>\n",
       "      <td>7d03a5db7166724ce98840ecb925bdab488e41e6</td>\n",
       "      <td>321d5a6d4f78f64b579f4182d18857e342da95c80dd30c...</td>\n",
       "      <td>51</td>\n",
       "      <td>46</td>\n",
       "      <td>True</td>\n",
       "      <td>exe</td>\n",
       "      <td>23:36.9</td>\n",
       "      <td>1</td>\n",
       "      <td>237766</td>\n",
       "      <td>5.571339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>.\\train\\1000</td>\n",
       "      <td>00a1a53ae5ed39cbc04df7bd5bb3f9d1</td>\n",
       "      <td>249042d8478015cefdcd114a2009db14c2c20d48</td>\n",
       "      <td>3a3d86e16b03f98d10e3bd28d69f8486b868ce107b3332...</td>\n",
       "      <td>54</td>\n",
       "      <td>51</td>\n",
       "      <td>True</td>\n",
       "      <td>exe</td>\n",
       "      <td>10:37.1</td>\n",
       "      <td>1</td>\n",
       "      <td>57856</td>\n",
       "      <td>7.860059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>.\\train\\1001</td>\n",
       "      <td>00a1abf0d976822bd049a72194af7440</td>\n",
       "      <td>cc96a187b2eca3f1b05fe0ed4c2ed7d005c3cb5a</td>\n",
       "      <td>e61e81190ca872eead63880a86a76f5c12563d51bd62d3...</td>\n",
       "      <td>51</td>\n",
       "      <td>40</td>\n",
       "      <td>True</td>\n",
       "      <td>exe</td>\n",
       "      <td>10:37.2</td>\n",
       "      <td>1</td>\n",
       "      <td>288256</td>\n",
       "      <td>7.016225</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           path                               md5  \\\n",
       "0     .\\train\\1  0000e1cccca9ca2ae5d1dfb124686920   \n",
       "1    .\\train\\10  000a1a3e8204261b7408ff51ffa5b440   \n",
       "2   .\\train\\100  013539d21eb2a5db8d0554affb3541b8   \n",
       "3  .\\train\\1000  00a1a53ae5ed39cbc04df7bd5bb3f9d1   \n",
       "4  .\\train\\1001  00a1abf0d976822bd049a72194af7440   \n",
       "\n",
       "                                       sha1  \\\n",
       "0  7704b8f6a9538f9bc2366696cf85eeec1f15e224   \n",
       "1  fb4a5e7801cf6d0f293b6051569e325c46271309   \n",
       "2  7d03a5db7166724ce98840ecb925bdab488e41e6   \n",
       "3  249042d8478015cefdcd114a2009db14c2c20d48   \n",
       "4  cc96a187b2eca3f1b05fe0ed4c2ed7d005c3cb5a   \n",
       "\n",
       "                                              sha256  total  positives  \\\n",
       "0  e1bde0b326a91688e8718086d06d1377394f2cb7feddec...     64         42   \n",
       "1  9d5fc038b67068fa5fba1bbb34b938db83b26638b4e22f...     52         23   \n",
       "2  321d5a6d4f78f64b579f4182d18857e342da95c80dd30c...     51         46   \n",
       "3  3a3d86e16b03f98d10e3bd28d69f8486b868ce107b3332...     54         51   \n",
       "4  e61e81190ca872eead63880a86a76f5c12563d51bd62d3...     51         40   \n",
       "\n",
       "   malicious filetype submitted  user_id  length   entropy  \n",
       "0       True      exe   40:22.2        1   67328  7.265160  \n",
       "1       True      exe   40:24.5        1  773024  7.935332  \n",
       "2       True      exe   23:36.9        1  237766  5.571339  \n",
       "3       True      exe   10:37.1        1   57856  7.860059  \n",
       "4       True      exe   10:37.2        1  288256  7.016225  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the CSV files as DataFrames\n",
    "df_train = pd.read_csv(csv_input_train_path)\n",
    "df_test = pd.read_csv(csv_input_test_path)\n",
    "\n",
    "# Rename `id` to `path` and `list` to `malicious`\n",
    "df_train = df_train.rename(columns={'id': 'path', 'list': 'malicious'})\n",
    "df_test = df_test.rename(columns={'id': 'path', 'list': 'malicious'})\n",
    "\n",
    "# Change `malicious` column to bool type\n",
    "df_train['malicious'] = df_train['malicious'].eq('Blacklist')\n",
    "df_test['malicious'] = df_test['malicious'].eq('Blacklist')\n",
    "\n",
    "# Change file name to relative path\n",
    "df_train['path'] = df_train['path'].apply(lambda path: os.path.join(dir_train, str(path)))\n",
    "df_test['path'] = df_test['path'].apply(lambda path: os.path.join(dir_test, str(path)))\n",
    "\n",
    "# Sort by path\n",
    "df_train = df_train.sort_values(by='path').reset_index(drop=True)\n",
    "df_test = df_test.sort_values(by='path').reset_index(drop=True)\n",
    "\n",
    "# Preview training data\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.\\train\\1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>.\\train\\10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>.\\train\\100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>.\\train\\1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>.\\train\\1001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           path\n",
       "0     .\\train\\1\n",
       "1    .\\train\\10\n",
       "2   .\\train\\100\n",
       "3  .\\train\\1000\n",
       "4  .\\train\\1001"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Store labels in new dataframe\n",
    "df_train_labels = df_train[['path', 'malicious']].copy()\n",
    "df_test_labels = df_test[['path', 'malicious']].copy()\n",
    "\n",
    "# Save labels to CSV\n",
    "df_train_labels.to_csv(csv_train_labels_path, index=False)\n",
    "df_test_labels.to_csv(csv_test_labels_path, index=False)\n",
    "\n",
    "# Store input data in new dataframe, saving only the `path` column\n",
    "df_train_data = df_train[['path']].copy()\n",
    "df_test_data = df_test[['path']].copy()\n",
    "\n",
    "# Preview training data\n",
    "df_train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '.\\\\train\\\\1'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# use magic numbers to find filetype\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmagic\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m df_train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mextension\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf_train\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpath\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmagic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmime\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m df_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mextension\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m path: magic\u001b[38;5;241m.\u001b[39mfrom_file(path, mime\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n",
      "File \u001b[1;32mc:\\Users\\Hannah\\Desktop\\T3\\.venv\\Lib\\site-packages\\pandas\\core\\series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4800\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4918\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4922\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m-> 4924\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Hannah\\Desktop\\T3\\.venv\\Lib\\site-packages\\pandas\\core\\apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Hannah\\Desktop\\T3\\.venv\\Lib\\site-packages\\pandas\\core\\apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[0;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Users\\Hannah\\Desktop\\T3\\.venv\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Hannah\\Desktop\\T3\\.venv\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1747\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[43], line 3\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# use magic numbers to find filetype\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmagic\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m df_train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mextension\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df_train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m path: \u001b[43mmagic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmime\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m)\n\u001b[0;32m      4\u001b[0m df_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mextension\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m path: magic\u001b[38;5;241m.\u001b[39mfrom_file(path, mime\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n",
      "File \u001b[1;32mc:\\Users\\Hannah\\Desktop\\T3\\.venv\\Lib\\site-packages\\magic\\magic.py:135\u001b[0m, in \u001b[0;36mfrom_file\u001b[1;34m(filename, mime)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\"\u001b[39;00m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;124;03mAccepts a filename and returns the detected filetype.  Return\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;124;03mvalue is the mimetype if mime=True, otherwise a human readable\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;124;03m'application/pdf'\u001b[39;00m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    134\u001b[0m m \u001b[38;5;241m=\u001b[39m _get_magic_type(mime)\n\u001b[1;32m--> 135\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Hannah\\Desktop\\T3\\.venv\\Lib\\site-packages\\magic\\magic.py:85\u001b[0m, in \u001b[0;36mMagic.from_file\u001b[1;34m(self, filename)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_file\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename):\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;66;03m# raise FileNotFoundException or IOError if the file does not exist\u001b[39;00m\n\u001b[1;32m---> 85\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     86\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlock:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '.\\\\train\\\\1'"
     ]
    }
   ],
   "source": [
    "# use magic numbers to find filetype\n",
    "import magic\n",
    "df_train['extension'] = df_train['path'].apply(lambda path: magic.from_file(path, mime=True))\n",
    "df_test['extension'] = df_test['path'].apply(lambda path: magic.from_file(path, mime=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'# train rows:               {len(df_train_data)}')\n",
    "print(f'# test rows:                {len(df_test_data)}')\n",
    "print(f'# NaN train path values:    {df_train_data['path'].isna().sum()}')\n",
    "print(f'# NaN test path values:     {df_test_data['path'].isna().sum()}')\n",
    "print(f'# unique train path values: {df_train_data['path'].nunique()}')\n",
    "print(f'# unique test path values:  {df_test_data['path'].nunique()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entropy(path):\n",
    "    byte_counts = np.zeros(256, dtype=np.uint64)\n",
    "    with open(path, 'rb') as file:\n",
    "        while chunk := file.read(DEFAULT_BUFFER_SIZE):\n",
    "            byte_counts += np.bincount(np.frombuffer(chunk, dtype=np.uint8), minlength=256).astype(np.uint64)\n",
    "    file_size = os.path.getsize(path)\n",
    "    return entropy(byte_counts / file_size, base=2) if file_size else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the Shannon entropies of the files\n",
    "tqdm.pandas()\n",
    "df_train_data['entropy'] = df_train_data['path'].progress_apply(get_entropy)\n",
    "df_test_data['entropy'] = df_test_data['path'].progress_apply(get_entropy)\n",
    "\n",
    "# Preview training data\n",
    "df_train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_pe(path):\n",
    "    d = dict()\n",
    "    try:\n",
    "        d = pefile.PE(path).dump_dict()\n",
    "        d.pop('LOAD_CONFIG', None)\n",
    "        d.pop('TLS', None)\n",
    "        d['Parsing Warnings'] = 'Parsing Warnings' in d\n",
    "        _keys = list(d.keys())\n",
    "        for key in _keys:\n",
    "            if isinstance(d[key], list):\n",
    "                d.pop(key, None)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'Error processing {path}: {e}')\n",
    "        d['Parsing Warnings'] = True\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make copies of the dataframes because the next operation takes a long time\n",
    "_temp_df_train_data = df_train_data.copy()\n",
    "_temp_df_test_data = df_test_data.copy()\n",
    "\n",
    "# Get the portable executable file information\n",
    "# This excludes LOAD_CONFIG and TLS data and columns whose values are lists\n",
    "_temp_train = _temp_df_train_data['path'].progress_apply(parse_pe)\n",
    "_temp_test = _temp_df_test_data['path'].progress_apply(parse_pe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the portable executable file information to the data\n",
    "df_train_data = pd.concat([_temp_df_train_data, pd.json_normalize(_temp_train)], axis=1)\n",
    "df_test_data = pd.concat([_temp_df_test_data, pd.json_normalize(_temp_test)], axis=1)\n",
    "\n",
    "# Replace all NaN values with 0\n",
    "df_train_data = df_train_data.fillna(0)\n",
    "df_test_data = df_test_data.fillna(0)\n",
    "\n",
    "# Exclude columns with 2 or fewer unique values\n",
    "_cols = df_train_data.columns[df_train_data.nunique() <= 2].tolist()\n",
    "df_train_data = df_train_data.drop(columns=_cols)\n",
    "df_test_data = df_test_data.drop(columns=_cols)\n",
    "\n",
    "# Exclude columns that end with 'Offset' (this removes a lot of useless data)\n",
    "_cols = [_col for _col in df_train_data.columns if _col.endswith('Offset')]\n",
    "df_train_data = df_train_data.drop(columns=_cols)\n",
    "df_test_data = df_test_data.drop(columns=_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview training data\n",
    "df_train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "# Some of the columns' data types are numerical, but many are strings (object)\n",
    "# We may want to one-hot encode the columns whose data types are not numeric\n",
    "# https://en.wikipedia.org/wiki/One-hot#Machine_learning_and_statistics\n",
    "df_train_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or... we can just drop non-numeric data for simplicity\n",
    "_cols = df_train_data.select_dtypes(exclude=np.number).drop(columns='path')\n",
    "df_train_data = df_train_data.drop(columns=_cols)\n",
    "df_test_data = df_test_data.drop(columns=_cols)\n",
    "\n",
    "# All of the columns should be numeric except for `path`\n",
    "df_train_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint progress\n",
    "df_train_data.to_csv(csv_train_data_path, index=False)\n",
    "df_test_data.to_csv(csv_test_data_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** IMPORTANT ***\n",
    "\n",
    "The cells above will generate the csv files needed for training and testing. After running the cells above once, you don't need to run them again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load save state\n",
    "df_train_data = pd.read_csv(csv_train_data_path)\n",
    "df_test_data = pd.read_csv(csv_test_data_path)\n",
    "\n",
    "df_train_labels = pd.read_csv(csv_train_labels_path)\n",
    "df_test_labels = pd.read_csv(csv_test_labels_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Z-score normalize all the data (except for `path`, which isn't numeric)\n",
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(df_train_data.drop(columns=['path']))\n",
    "x_test = scaler.transform(df_test_data.drop(columns=['path']))\n",
    "\n",
    "# Preview the input data\n",
    "x_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the training labels\n",
    "y_train = np.array(df_train_labels['malicious'])\n",
    "y_test = np.array(df_test_labels['malicious'])\n",
    "\n",
    "# Verify that the training and testing datasets are balanced\n",
    "print(df_train_labels['malicious'].value_counts(), end='\\n\\n')\n",
    "print(df_test_labels['malicious'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'Adaptive Boosting Classifier': AdaBoostClassifier(random_state=seed),\n",
    "    'GBM': GradientBoostingClassifier(random_state=seed),\n",
    "    'Histogram-based GBM': HistGradientBoostingClassifier(random_state=seed),\n",
    "    'Random Forest Classifier': RandomForestClassifier(random_state=seed),\n",
    "    'MLP': MLPClassifier(max_iter=1000, random_state=seed),\n",
    "    'C-SVC': SVC(random_state=seed),\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Train the model\n",
    "    model = model.fit(x_train, y_train)\n",
    "    # Test the model\n",
    "    y_pred = model.predict(x_test)\n",
    "    # Print the results of the test\n",
    "    print(f'{name}: {accuracy_score(y_test, y_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scan_file(filename):\n",
    "    #grab file object, give to AI, return either malware or not\n",
    "    output = filename #placeholder line\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout = [[sg.Text('What file do you want to scan?')], \n",
    "          [sg.Input(key='-IN-', enable_events=True), sg.FileBrowse(target='-IN-', initial_folder='Downloads')],\n",
    "          [sg.Text(key='-OUT-', size=(50,10), text='')],\n",
    "          [sg.Button('Exit')]]\n",
    "window = sg.Window('File Scan', layout)\n",
    "\n",
    "while True:\n",
    "    event, values = window.read()\n",
    "    if event == sg.WIN_CLOSED or event == 'Exit':\n",
    "        break\n",
    "    if event == '-IN-':\n",
    "        filename = values[event]\n",
    "        output = scan_file(filename)\n",
    "        window['-OUT-'].update(output)\n",
    "\n",
    "window.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
