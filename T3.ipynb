{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from io import DEFAULT_BUFFER_SIZE\n",
    "from scipy.stats import entropy\n",
    "from tqdm.auto import tqdm\n",
    "import pefile\n",
    "import PySimpleGUI as sg\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "seed = 42\n",
    "# Path to original train.csv\n",
    "csv_input_train_path = 'train.csv'\n",
    "# Path to original test.csv\n",
    "csv_input_test_path = 'test.csv'\n",
    "# Path to training data\n",
    "csv_train_data_path = 'train-data.csv'\n",
    "# Path to testing data\n",
    "csv_test_data_path = 'test-data.csv'\n",
    "# Path to training labels\n",
    "csv_train_labels_path = 'train-labels.csv'\n",
    "# Path to testing labels\n",
    "csv_test_labels_path = 'test-labels.csv'\n",
    "# Path to training samples directory\n",
    "dir_train = 'train'\n",
    "# Path to testing samples directory\n",
    "dir_test = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV files as DataFrames\n",
    "df_train = pd.read_csv(csv_input_train_path)\n",
    "df_test = pd.read_csv(csv_input_test_path)\n",
    "\n",
    "# Rename `id` to `path` and `list` to `malicious`\n",
    "df_train = df_train.rename(columns={'id': 'path', 'list': 'malicious'})\n",
    "df_test = df_test.rename(columns={'id': 'path', 'list': 'malicious'})\n",
    "\n",
    "# Change `malicious` column to bool type\n",
    "df_train['malicious'] = df_train['malicious'].eq('Blacklist')\n",
    "df_test['malicious'] = df_test['malicious'].eq('Blacklist')\n",
    "\n",
    "# Change file name to relative path\n",
    "df_train['path'] = df_train['path'].apply(lambda path: os.path.join(dir_train, str(path)))\n",
    "df_test['path'] = df_test['path'].apply(lambda path: os.path.join(dir_test, str(path)))\n",
    "\n",
    "# Sort by path\n",
    "df_train = df_train.sort_values(by='path').reset_index(drop=True)\n",
    "df_test = df_test.sort_values(by='path').reset_index(drop=True)\n",
    "\n",
    "# Preview training data\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store labels in new dataframe\n",
    "df_train_labels = df_train[['path', 'malicious']].copy()\n",
    "df_test_labels = df_test[['path', 'malicious']].copy()\n",
    "\n",
    "# Save labels to CSV\n",
    "df_train_labels.to_csv(csv_train_labels_path, index=False)\n",
    "df_test_labels.to_csv(csv_test_labels_path, index=False)\n",
    "\n",
    "# Store input data in new dataframe, saving only the `path` column\n",
    "df_train_data = df_train[['path']].copy()\n",
    "df_test_data = df_test[['path']].copy()\n",
    "\n",
    "# Preview training data\n",
    "df_train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use magic numbers to find filetype\n",
    "import magic\n",
    "\n",
    "tqdm.pandas()\n",
    "df_train_data['extension'] = df_train_data['path'].progress_apply(lambda path: magic.from_file(path, mime=True))\n",
    "df_test_data['extension'] = df_test_data['path'].progress_apply(lambda path: magic.from_file(path, mime=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'# train rows:               {len(df_train_data)}')\n",
    "print(f'# test rows:                {len(df_test_data)}')\n",
    "print(f\"# NaN train path values:    {df_train_data['path'].isna().sum()}\")\n",
    "print(f\"# NaN test path values:     {df_test_data['path'].isna().sum()}\")\n",
    "print(f\"# unique train path values: {df_train_data['path'].nunique()}\")\n",
    "print(f\"# unique test path values:  {df_test_data['path'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entropy(path):\n",
    "    byte_counts = np.zeros(256, dtype=np.uint64)\n",
    "    with open(path, 'rb') as file:\n",
    "        while chunk := file.read(DEFAULT_BUFFER_SIZE):\n",
    "            byte_counts += np.bincount(np.frombuffer(chunk, dtype=np.uint8), minlength=256).astype(np.uint64)\n",
    "    file_size = os.path.getsize(path)\n",
    "    return entropy(byte_counts / file_size, base=2) if file_size else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the Shannon entropies of the files\n",
    "df_train_data['entropy'] = df_train_data['path'].progress_apply(get_entropy)\n",
    "df_test_data['entropy'] = df_test_data['path'].progress_apply(get_entropy)\n",
    "\n",
    "# Preview training data\n",
    "df_train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_pe(path):\n",
    "    d = dict()\n",
    "    try:\n",
    "        d = pefile.PE(path).dump_dict()\n",
    "        d.pop('LOAD_CONFIG', None)\n",
    "        d.pop('TLS', None)\n",
    "        d['Parsing Warnings'] = 'Parsing Warnings' in d\n",
    "        _keys = list(d.keys())\n",
    "        for key in _keys:\n",
    "            if isinstance(d[key], list):\n",
    "                d.pop(key, None)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'Error processing {path}: {e}')\n",
    "        d['Parsing Warnings'] = True\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make copies of the dataframes because the next operation takes a long time\n",
    "_temp_df_train_data = df_train_data.copy()\n",
    "_temp_df_test_data = df_test_data.copy()\n",
    "\n",
    "# Get the portable executable file information\n",
    "# This excludes LOAD_CONFIG and TLS data and columns whose values are lists\n",
    "_temp_train = _temp_df_train_data['path'].progress_apply(parse_pe)\n",
    "_temp_test = _temp_df_test_data['path'].progress_apply(parse_pe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the portable executable file information to the data\n",
    "df_train_data = pd.concat([_temp_df_train_data, pd.json_normalize(_temp_train)], axis=1)\n",
    "df_test_data = pd.concat([_temp_df_test_data, pd.json_normalize(_temp_test)], axis=1)\n",
    "\n",
    "# Replace all NaN values with 0\n",
    "df_train_data = df_train_data.fillna(0)\n",
    "df_test_data = df_test_data.fillna(0)\n",
    "\n",
    "# Exclude columns with 2 or fewer unique values\n",
    "_cols = df_train_data.columns[df_train_data.nunique() <= 2].tolist()\n",
    "df_train_data = df_train_data.drop(columns=_cols)\n",
    "df_test_data = df_test_data.drop(columns=_cols)\n",
    "\n",
    "# Exclude columns that end with 'Offset' (this removes a lot of useless data)\n",
    "_cols = [_col for _col in df_train_data.columns if _col.endswith('Offset')]\n",
    "df_train_data = df_train_data.drop(columns=_cols)\n",
    "df_test_data = df_test_data.drop(columns=_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview training data\n",
    "df_train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some of the columns' data types are numerical, but many are strings (object)\n",
    "# We may want to one-hot encode the columns whose data types are not numeric\n",
    "# https://en.wikipedia.org/wiki/One-hot#Machine_learning_and_statistics\n",
    "df_train_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or... we can just drop non-numeric data for simplicity\n",
    "_cols = df_train_data.select_dtypes(exclude=np.number).drop(columns='path')\n",
    "df_train_data = df_train_data.drop(columns=_cols)\n",
    "df_test_data = df_test_data.drop(columns=_cols)\n",
    "\n",
    "# All of the columns should be numeric except for `path`\n",
    "df_train_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint progress\n",
    "df_train_data.to_csv(csv_train_data_path, index=False)\n",
    "df_test_data.to_csv(csv_test_data_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** IMPORTANT ***\n",
    "\n",
    "The cells above will generate the csv files needed for training and testing. After running the cells above once, you don't need to run them again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load save state\n",
    "df_train_data = pd.read_csv(csv_train_data_path)\n",
    "df_test_data = pd.read_csv(csv_test_data_path)\n",
    "\n",
    "df_train_labels = pd.read_csv(csv_train_labels_path)\n",
    "df_test_labels = pd.read_csv(csv_test_labels_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Z-score normalize all the data (except for `path`, which isn't numeric)\n",
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(df_train_data.drop(columns=['path']))\n",
    "x_test = scaler.transform(df_test_data.drop(columns=['path']))\n",
    "\n",
    "# Preview the input data\n",
    "x_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the training labels\n",
    "y_train = np.array(df_train_labels['malicious'])\n",
    "y_test = np.array(df_test_labels['malicious'])\n",
    "\n",
    "# Verify that the training and testing datasets are balanced\n",
    "print(df_train_labels['malicious'].value_counts(), end='\\n\\n')\n",
    "print(df_test_labels['malicious'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'Adaptive Boosting Classifier': AdaBoostClassifier(random_state=seed),\n",
    "    'GBM': GradientBoostingClassifier(random_state=seed),\n",
    "    'Histogram-based GBM': HistGradientBoostingClassifier(random_state=seed),\n",
    "    'Random Forest Classifier': RandomForestClassifier(random_state=seed),\n",
    "    'MLP': MLPClassifier(max_iter=1000, random_state=seed),\n",
    "    'C-SVC': SVC(random_state=seed),\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Train the model\n",
    "    model = model.fit(x_train, y_train)\n",
    "    # Test the model\n",
    "    y_pred = model.predict(x_test)\n",
    "    # Print the results of the test\n",
    "    print(f'{name}: {accuracy_score(y_test, y_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scan_file(filename):\n",
    "    #grab file object, give to AI, return either malware or not\n",
    "    output = filename #placeholder line\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout = [[sg.Text('What file do you want to scan?')], \n",
    "          [sg.Input(key='-IN-', enable_events=True), sg.FileBrowse(target='-IN-', initial_folder='Downloads')],\n",
    "          [sg.Text(key='-OUT-', size=(50,10), text='')],\n",
    "          [sg.Button('Exit')]]\n",
    "window = sg.Window('File Scan', layout)\n",
    "\n",
    "while True:\n",
    "    event, values = window.read()\n",
    "    if event == sg.WIN_CLOSED or event == 'Exit':\n",
    "        break\n",
    "    if event == '-IN-':\n",
    "        filename = values[event]\n",
    "        output = scan_file(filename)\n",
    "        window['-OUT-'].update(output)\n",
    "\n",
    "window.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
